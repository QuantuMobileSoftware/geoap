{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting boundaries for given AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "import tempfile\n",
    "import shapely\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import join, basename, split\n",
    "from skimage import measure\n",
    "from scipy.ndimage import rotate\n",
    "from rasterio.features import rasterize, shapes\n",
    "from shapely.geometry import Polygon, shape\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from sentinel2download.downloader import Sentinel2Downloader\n",
    "from preprocessing import preprocess_sentinel_raw_data, read_raster, extract_tci\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(aoi_path, sentinel_tiles_path):\n",
    "    '''\n",
    "    Returns Sentinel-2 tiles that intersects with specified AoI.\n",
    "\n",
    "        Parameters:\n",
    "            aoi_path (str): Path to geojson/shp file with AoI to process.\n",
    "            sentinel_tiles_path (str): Path to geojson/shp file with all Sentinel-2 tiles.\n",
    "\n",
    "        Returns:\n",
    "            date_tile_info (GeoDataFrame): Filtered tiles (tileID, geometry, date).\n",
    "    '''\n",
    "    aoi_file = gpd.read_file(aoi_path)\n",
    "    sentinel_tiles = gpd.read_file(sentinel_tiles_path)\n",
    "    sentinel_tiles.set_index(\"Name\", drop=False, inplace=True)\n",
    "\n",
    "    best_interseciton = {\"tileID\": [], \"geometry\": []}\n",
    "    rest_aoi = aoi_file.copy()\n",
    "\n",
    "    while rest_aoi.area.sum() > 0:\n",
    "        res_intersection = gpd.overlay(rest_aoi, sentinel_tiles, how=\"intersection\")\n",
    "        biggest_area_idx = res_intersection.area.argmax()\n",
    "\n",
    "        tileID = res_intersection.loc[biggest_area_idx, \"Name\"]\n",
    "        this_aoi = res_intersection.loc[biggest_area_idx, \"geometry\"]\n",
    "\n",
    "        best_interseciton[\"tileID\"].append(tileID)\n",
    "        best_interseciton[\"geometry\"].append(this_aoi)\n",
    "\n",
    "        biggest_intersection = sentinel_tiles.loc[[tileID]]\n",
    "        rest_aoi = gpd.overlay(rest_aoi, biggest_intersection, how=\"difference\")\n",
    "        sentinel_tiles = sentinel_tiles.loc[res_intersection[\"Name\"]]\n",
    "\n",
    "    date_tile_info = gpd.GeoDataFrame(best_interseciton)\n",
    "    date_tile_info.crs = aoi_file.crs\n",
    "    \n",
    "    return date_tile_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segmentation_mask_with_multiple_th(image_path, thresholds, min_edge_size, min_obj_size, th_size=(11, 3)):\n",
    "    '''\n",
    "    Run segmentation on the given raster to find edges. \n",
    "    Combine multiple predictions with different thresholds with cv2.adaptiveThreshold.\n",
    "\n",
    "        Parameters:\n",
    "            image_path (str): Path to raster which will be processed.\n",
    "            thresholds (:obj:`list` of :obj:`float`): List of thresholds used to filter weak edges.\n",
    "            min_edge_size (int): Edges with lower size will be filtered.\n",
    "            min_obj_size (int): Instances with lower size will be filtered.\n",
    "            th_size (:obj:`tuple` of :obj:`int`): Size of cv2.adaptiveThreshold filter.\n",
    "\n",
    "        Returns:\n",
    "            img (numpy.array): Binary image with segmented instances.\n",
    "            meta (dict): Source raster metadata\n",
    "    '''\n",
    "    bands, meta = read_raster(image_path)\n",
    "    results = [None] * len(thresholds)\n",
    "    for i, th in enumerate(thresholds):\n",
    "        results[i], _ = find_segmentation_mask(image_path, th, min_edge_size, min_obj_size)\n",
    "    combined = np.mean(results, 0).astype(np.uint8)\n",
    "    thresholded = cv2.adaptiveThreshold(\n",
    "        combined,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        *th_size\n",
    "    )\n",
    "    labeled = label_detected_instances(thresholded, connectivity=1)\n",
    "    labeled = remove_small_objects(labeled, min_obj_size / 10)\n",
    "    labeled = remove_background(labeled, bands)\n",
    "    labeled[thresholded == 0] = 0\n",
    "    img = convert_to_binary(labeled)\n",
    "    \n",
    "    return img, meta\n",
    "\n",
    "\n",
    "def find_segmentation_mask(image_path, edge_combination_th, min_edge_size, min_obj_size):\n",
    "    '''\n",
    "    Run segmentation on the given raster to find edges.\n",
    "\n",
    "        Parameters:\n",
    "            image_path (str): Path to raster which will be processed.\n",
    "            edge_combination_th (float): Threshold that used to filter weak edges.\n",
    "            min_edge_size (int): Edges with lower size will be filtered.\n",
    "            min_obj_size (int): Instances with lower size will be filtered.\n",
    "\n",
    "        Returns:\n",
    "            binary_img (numpy.array): Binary image with segmented instances.\n",
    "            meta (dict): Source raster metadata\n",
    "    '''\n",
    "    bands, meta = read_raster(image_path)\n",
    "    avg_std = compute_image_standard_deviation(bands)\n",
    "    filter_list = create_edging_filters()\n",
    "    edges = apply_edging_filters(avg_std, filter_list)\n",
    "    edge_direction_list = combine_edges_layers(avg_std, edges, edge_combination_th)\n",
    "    edge_direction_list = remove_short_edges(edge_direction_list, min_edge_size)\n",
    "    binary_mask = edges_union(edge_direction_list)\n",
    "    labeled_image = label_detected_instances(binary_mask)\n",
    "    labeled_image = remove_small_objects(labeled_image, min_obj_size)\n",
    "    labeled_image = remove_background(labeled_image, bands)\n",
    "    binary_img = convert_to_binary(labeled_image)\n",
    "\n",
    "    return binary_img, meta\n",
    "\n",
    "\n",
    "def compute_image_standard_deviation(bands, kernel_size=(5, 5)):\n",
    "    std_list = [\n",
    "        find_edges_with_standard_deviation(b, kernel_size) for b in bands\n",
    "    ]\n",
    "    return np.mean(std_list, 0)\n",
    "\n",
    "\n",
    "def find_edges_with_standard_deviation(sample, filter_size=(3, 3)):\n",
    "    mean = cv2.blur(sample, filter_size)\n",
    "    mean_sqr = cv2.blur(sample * sample, filter_size)\n",
    "    std = cv2.sqrt(mean_sqr - mean*mean)\n",
    "    return std\n",
    "\n",
    "\n",
    "def create_edging_filters(length=13, count=16):\n",
    "    base_filter = np.array([\n",
    "        [-1] * length,\n",
    "        [1] * length,\n",
    "        [0] * length,\n",
    "    ])\n",
    "    filter_list = [None] * count * 2\n",
    "    filter_list[0] = base_filter\n",
    "    step = 180 / count\n",
    "\n",
    "    for i in range(count // 2):\n",
    "        filter_ = rotate(base_filter, i * step, order=0)\n",
    "        filter_list[i*4] = filter_\n",
    "        for j in range(1, 4):\n",
    "            filter_list[i*4+j] = np.rot90(filter_, j)\n",
    "\n",
    "    return filter_list\n",
    "\n",
    "\n",
    "def apply_edging_filters(sample, filter_list):\n",
    "    return [cv2.filter2D(sample, -1, f) for f in filter_list]\n",
    "\n",
    "\n",
    "def combine_edges_layers(avg_std, edges, th):\n",
    "    filter_count = len(edges) // 2\n",
    "    local_max_left = [None] * filter_count\n",
    "    local_max_right = [None] * filter_count\n",
    "\n",
    "    for i in range(filter_count // 2):\n",
    "        local_max_left[2*i] = cv2.bitwise_and(avg_std, edges[4*i])\n",
    "        local_max_left[2*i+1] = cv2.bitwise_and(avg_std, edges[4*i+1])\n",
    "        local_max_right[2*i] = cv2.bitwise_and(avg_std, edges[4*i+2])\n",
    "        local_max_right[2*i+1] = cv2.bitwise_and(avg_std, edges[4*i+3])\n",
    "\n",
    "    local_max_left = np.asanyarray(local_max_left)\n",
    "    local_max_right = np.asanyarray(local_max_right)\n",
    "\n",
    "    combined_result = np.zeros_like(local_max_left, np.uint8)\n",
    "    combined_result[\n",
    "        (local_max_left > 0) &\n",
    "        (local_max_right > 0) &\n",
    "        (local_max_left + local_max_right > th)\n",
    "    ] = 1\n",
    "\n",
    "    return combined_result\n",
    "\n",
    "\n",
    "def remove_short_edges(edge_direction_list, min_edge_size):\n",
    "    res = [None] * len(edge_direction_list)\n",
    "    for i, edge_direction in enumerate(edge_direction_list):\n",
    "        labeled_image = label_detected_instances(edge_direction, 0, 2)\n",
    "        labels, counts_labels = np.unique(labeled_image, return_counts=True)\n",
    "        edge_direction = np.isin(\n",
    "            labeled_image,\n",
    "            labels[counts_labels > min_edge_size]\n",
    "        ) & (edge_direction > 0)\n",
    "\n",
    "        res[i] = edge_direction.astype(np.uint8)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def edges_union(edges):\n",
    "    result = edges[0]\n",
    "    for e in edges[1:]:\n",
    "        result = cv2.bitwise_or(result, e)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def label_detected_instances(binary_mask, background=1, connectivity=2):\n",
    "    return measure.label(\n",
    "        binary_mask,\n",
    "        background=background,\n",
    "        connectivity=connectivity\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_small_objects(labeled_image, min_obj_size):\n",
    "    labels, counts_labels = np.unique(labeled_image, return_counts=True)\n",
    "    labeled_image[np.isin(\n",
    "        labeled_image,\n",
    "        labels[counts_labels < min_obj_size]\n",
    "    )] = 0\n",
    "    return labeled_image\n",
    "\n",
    "\n",
    "def remove_background(prediction, bands):\n",
    "    mask = np.sum(bands, axis=0) == 0\n",
    "    prediction[mask] = 0\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def convert_to_binary(img):\n",
    "    img[img > 0] = 255\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def polygonize(binary_img, meta, transform=True):\n",
    "    polygons = shapes(\n",
    "        binary_img, \n",
    "        binary_img, \n",
    "        transform=meta[\"transform\"],\n",
    "        connectivity=8\n",
    "    )\n",
    "    return [shape(poly) for poly, _ in polygons]\n",
    "\n",
    "\n",
    "def process_polygons(result_df, current_crs, limit=500, dst_crs=\"EPSG:4326\"):\n",
    "    \"\"\"\n",
    "    Prepare result Dataframe with polygons\n",
    "\n",
    "        Parameters:\n",
    "            result_df (pd.DataFrame): Result DataFrame\n",
    "            limit (int): min area for polygon in m2\n",
    "        Returns:\n",
    "            GeoDataFrame: GeoDataFrame ready for saving\n",
    "    \"\"\"\n",
    "        \n",
    "    gdf = gpd.GeoDataFrame(result_df)\n",
    "    gdf.crs = current_crs\n",
    "    \n",
    "    # fix invalid polygons\n",
    "    # gdf[\"geometry\"] = gdf.apply(lambda row: row.geometry.buffer(0), axis=1)\n",
    "    # select only valid polygons\n",
    "    # gdf = gdf.loc[gdf.is_valid]\n",
    "\n",
    "    # print(gdf.crs)\n",
    "    # TODO: can be uncomment if needed\n",
    "    # gdf = gdf.loc[gdf.area >= limit]\n",
    "    \n",
    "    gdf.to_crs(dst_crs, inplace=True)\n",
    "    # expand each polygon\n",
    "    # gdf.geometry = gdf.geometry.buffer(0.0001, 1)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def save_polygons(gdf, save_path):\n",
    "    if len(gdf) == 0:\n",
    "        print('No polygons detected.')\n",
    "        return\n",
    "\n",
    "    directory = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    save_path = save_path + \".temp\" \n",
    "    gdf.to_file(save_path, driver='GeoJSON')\n",
    "    os.rename(save_path, save_path[:-5])\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find tile indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = f\"/home/{os.getenv('NB_USER')}/work\"\n",
    "\n",
    "API_KEY = os.path.join(BASE, \".secret/sentinel2_google_api_key.json\")\n",
    "LOAD_DIR = os.path.join(BASE, \"satellite_imagery\")\n",
    "RESULTS_DIR = os.path.join(BASE, \"results/pbd\")\n",
    "\n",
    "PBD_DIR = os.path.join(BASE, \"notebooks/pbd\")\n",
    "\n",
    "BANDS = {'TCI', 'B08', }\n",
    "\n",
    "CONSTRAINTS = {'NODATA_PIXEL_PERCENTAGE': 15.0, 'CLOUDY_PIXEL_PERCENTAGE': 40.0, }\n",
    "# CONSTRAINTS = {'NODATA_PIXEL_PERCENTAGE': 10.0, 'CLOUDY_PIXEL_PERCENTAGE': 5.0, }\n",
    "\n",
    "PRODUCT_TYPE = 'L1C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(PBD_DIR, \"data\")):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_path = os.getenv(\"AOI\", os.path.join(PBD_DIR, \"data/20180410_082051_0f42_aoi.geojson\"))     \n",
    "aoi_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2018-04-08\"\n",
    "END_DATE = \"2018-04-12\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_tiles_path = os.path.join(BASE, \"grids/sentinel2grid.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find overlap tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_tile_info = get_tiles(aoi_path, sentinel_tiles_path)\n",
    "date_tile_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_folder(tile_folder, file, limit, nodata):\n",
    "    with rasterio.open(os.path.join(tile_folder, file)) as src:              \n",
    "        # Read in image as a numpy array\n",
    "        array = src.read(1)\n",
    "        # Count the occurance of NoData values in np array\n",
    "        nodata_count = np.count_nonzero(array == nodata)\n",
    "        # Get a % of NoData pixels\n",
    "        nodata_percentage = round(nodata_count / array.size * 100, 2)\n",
    "        print(f\"NODATA_PIXEL_PERCENTAGE for {tile_folder} images: {nodata_percentage}%\")\n",
    "        if nodata_percentage <= limit:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata(loadings, product_type, limit=15.0, nodata=0):\n",
    "    filtered = dict()\n",
    "    \n",
    "    print(f\"Checking NODATA_PIXEL_PERCENTAGE for {product_type}\") \n",
    "\n",
    "    for tile, tile_paths in loadings.items():\n",
    "        try:\n",
    "            for tile_path in tile_paths:\n",
    "                tile_folder = Path(tile_path[0]).parent\n",
    "                print(tile_folder)\n",
    "                if product_type == 'L1C' and limit:\n",
    "                     if _check_folder(tile_folder, limit, nodata):\n",
    "                        filtered[tile] = tile_folder\n",
    "                else:\n",
    "                    filtered[tile] = tile_folder\n",
    "        except Exception as ex:\n",
    "            print(f\"Error for {tile}: {str(ex)}\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(api_key, tiles, start_date, end_date, output_dir, product_type=\"L2A\"):\n",
    "    loader = Sentinel2Downloader(api_key)\n",
    "    loadings = dict()\n",
    "    for tile in tiles:\n",
    "        print(f\"Loading images for tile: {tile}...\")\n",
    "        loaded = loader.download(product_type,\n",
    "                                 [tile],\n",
    "                                 start_date=start_date,\n",
    "                                 end_date=end_date,\n",
    "                                 output_dir=output_dir,                       \n",
    "                                 bands=BANDS,\n",
    "                                constraints=CONSTRAINTS)\n",
    "        \n",
    "        print(f\"Loading images for tile {tile} finished\")\n",
    "        loadings[tile] = loaded\n",
    "    \n",
    "    tile_folders = dict()\n",
    "    for tile, tile_paths in loadings.items():\n",
    "        tile_folders[tile] = {str(Path(tile_path[0]).parent) for tile_path in tile_paths}\n",
    "    return tile_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = load_images(API_KEY, date_tile_info.tileID.values, START_DATE, END_DATE, LOAD_DIR, PRODUCT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if product type == L1C, check images NODATA_PIXEL_PERCENTAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata(loadings, product_type, limit=15.0, nodata=0):\n",
    "    filtered = dict()\n",
    "    \n",
    "    print(f\"Checking NODATA_PIXEL_PERCENTAGE for {product_type}\")            \n",
    "    \n",
    "    for tile, folders in loadings.items():\n",
    "        filtered_folders = set()\n",
    "        for folder in folders:\n",
    "            # print(folder)\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(\".jp2\") and \"OPER\" not in file:\n",
    "                    if product_type == 'L1C' and limit:\n",
    "                         if _check_folder(folder, file, limit, nodata):\n",
    "                            filtered_folders.add(folder)\n",
    "                            break\n",
    "                    else:\n",
    "                        filtered_folders.add(folder)\n",
    "        filtered[tile] = filtered_folders\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = check_nodata(loadings, PRODUCT_TYPE)\n",
    "checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select last folder with last date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_date(loadings):\n",
    "    def _find_last_date(folders):        \n",
    "        dates = list()\n",
    "        for folder in folders:        \n",
    "            search = re.search(r\"_(\\d+)T\\d+_\", str(folder))\n",
    "            date = search.group(1)\n",
    "            date = datetime.strptime(date, '%Y%m%d')\n",
    "            dates.append(date)    \n",
    "        last_date = max(dates)\n",
    "        last_date = datetime.strftime(last_date, '%Y%m%d')\n",
    "        return last_date\n",
    "    \n",
    "    filtered = dict()\n",
    "    for tile, folders in loadings.items():\n",
    "        try:\n",
    "            last_date = _find_last_date(folders)\n",
    "            for folder in folders:\n",
    "                if last_date in folder:\n",
    "                    filtered[tile] = folder\n",
    "        except Exception as ex:\n",
    "            print(f\"Error for {tile}: {str(ex)}\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter_date(checked)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered:\n",
    "    raise ValueError(\"Images not loaded. Change date or constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = [5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "min_edge_size = 200\n",
    "min_obj_size = 2000\n",
    "\n",
    "origin_name = os.path.basename(aoi_path).replace(\".geojson\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([])\n",
    "\n",
    "with tempfile.TemporaryDirectory(dir=PBD_DIR) as tmpdirname:   \n",
    "    for i, tile in tqdm(date_tile_info.iterrows(), total=date_tile_info.shape[0]):\n",
    "        try:\n",
    "            tile_folder = Path(filtered[tile.tileID])\n",
    "            print(tile_folder)\n",
    "        except Exception as ex:\n",
    "            print(f\"Error for {tile.tileID}: {str(ex)}\")\n",
    "        else:\n",
    "            print(f\"Processing {tile.tileID}...\")\n",
    "    \n",
    "            raster_path = preprocess_sentinel_raw_data(\n",
    "                save_path=tmpdirname,\n",
    "                tile_folder=tile_folder,\n",
    "                aoi_mask=date_tile_info.loc[[i]]\n",
    "            )\n",
    "            \n",
    "            tci_temp_path = os.path.join(tmpdirname, f\"{origin_name}_cropped_TCI.tif\")\n",
    "            extract_tci(raster_path, tci_temp_path, f\"{RESULTS_DIR}\")\n",
    "    \n",
    "            if len(th) == 1:\n",
    "                img, meta = find_segmentation_mask(\n",
    "                    raster_path, th[0],\n",
    "                    min_edge_size, min_obj_size\n",
    "                )\n",
    "            else:\n",
    "                img, meta = find_segmentation_mask_with_multiple_th(\n",
    "                    raster_path, th,\n",
    "                    min_edge_size, min_obj_size\n",
    "                )\n",
    "                df = pd.DataFrame({\"geometry\": polygonize(img, meta)})\n",
    "                df[\"id\"] = pd.Series(map(lambda x: f\"{origin_name}_{x}\", df.index.values))\n",
    "                df[\"tileID\"] = tile.tileID\n",
    "                df[\"start_date\"] = START_DATE\n",
    "                df[\"end_date\"] = END_DATE\n",
    "        \n",
    "\n",
    "            result_df = pd.concat([result_df, df])\n",
    "            \n",
    "            print(f\"Finished processing {tile.tileID}\")\n",
    "    # print(result_df)\n",
    "\n",
    "\n",
    "gdf = process_polygons(result_df, meta['crs'])\n",
    "save_path = os.path.join(RESULTS_DIR, f\"{origin_name}_prediction.geojson\")\n",
    "print(save_path)\n",
    "save_polygons(gdf, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
