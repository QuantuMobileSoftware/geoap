{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOI = 'POLYGON ((-85.299088 40.339368, -85.332047 40.241477, -85.134979 40.229427, -85.157639 40.34146, -85.299088 40.339368))'\n",
    "START_DATE = \"2020-05-01\"\n",
    "END_DATE = \"2020-06-30\"\n",
    "\n",
    "REQUEST_ID = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting boundaries for given AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "import tempfile\n",
    "import shapely\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import join, basename, split\n",
    "from skimage import measure\n",
    "from scipy.ndimage import rotate\n",
    "from rasterio.features import rasterize, shapes\n",
    "from rasterio.merge import merge\n",
    "from shapely.geometry import Polygon, shape, LinearRing\n",
    "import shapely.wkt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from sentinel2download.downloader import Sentinel2Downloader\n",
    "from sip_plot_boundary_detection_nn.code.preprocessing import (\n",
    "    preprocess_sentinel_raw_data, read_raster, extract_tci)\n",
    "from sip_plot_boundary_detection_nn.code.engine import load_model, val_tfs\n",
    "from sip_plot_boundary_detection_nn.code.dataset import BoundaryDetector\n",
    "from sip_plot_boundary_detection_nn.code.filter_polygons import filter_polygons, filter_sindex\n",
    "from sip_plot_boundary_detection_nn.code.utils import transform_crs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_crs = 'EPSG:4326'\n",
    "\n",
    "polygon = shapely.wkt.loads(AOI)\n",
    "aoi_filename = f\"{time.time()}_aoi.geojson\"\n",
    "gpd.GeoDataFrame(gpd.GeoSeries([polygon]), columns=[\"geometry\"]).to_file(aoi_filename, driver=\"GeoJSON\")\n",
    "start_date = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(END_DATE, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(aoi_path, sentinel_tiles_path):\n",
    "    '''\n",
    "    Returns Sentinel-2 tiles that intersects with specified AoI.\n",
    "\n",
    "        Parameters:\n",
    "            aoi_path (str): Path to geojson/shp file with AoI to process.\n",
    "            sentinel_tiles_path (str): Path to geojson/shp file with all Sentinel-2 tiles.\n",
    "\n",
    "        Returns:\n",
    "            date_tile_info (GeoDataFrame): Filtered tiles (tileID, geometry, date).\n",
    "    '''\n",
    "    aoi_file = gpd.read_file(aoi_path)\n",
    "    sentinel_tiles = gpd.read_file(sentinel_tiles_path)\n",
    "    sentinel_tiles.set_index(\"Name\", drop=False, inplace=True)\n",
    "\n",
    "    best_interseciton = {\"tileID\": [], \"geometry\": []}\n",
    "    rest_aoi = aoi_file.copy()\n",
    "\n",
    "    while rest_aoi.area.sum() > 0:\n",
    "        res_intersection = gpd.overlay(rest_aoi, sentinel_tiles, how=\"intersection\")\n",
    "        biggest_area_idx = res_intersection.area.argmax()\n",
    "\n",
    "        tileID = res_intersection.loc[biggest_area_idx, \"Name\"]\n",
    "        this_aoi = res_intersection.loc[biggest_area_idx, \"geometry\"]\n",
    "\n",
    "        best_interseciton[\"tileID\"].append(tileID)\n",
    "        best_interseciton[\"geometry\"].append(this_aoi)\n",
    "\n",
    "        biggest_intersection = sentinel_tiles.loc[[tileID]]\n",
    "        rest_aoi = gpd.overlay(rest_aoi, biggest_intersection, how=\"difference\")\n",
    "        sentinel_tiles = sentinel_tiles.loc[res_intersection[\"Name\"]]\n",
    "\n",
    "    date_tile_info = gpd.GeoDataFrame(best_interseciton)\n",
    "    date_tile_info.crs = aoi_file.crs\n",
    "    \n",
    "    return date_tile_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_polygons(result_df, current_crs, limit=500, dst_crs=\"EPSG:4326\"):\n",
    "    \"\"\"\n",
    "    Prepare result Dataframe with polygons\n",
    "\n",
    "        Parameters:\n",
    "            result_df (pd.DataFrame): Result DataFrame\n",
    "            limit (int): min area for polygon in m2\n",
    "        Returns:\n",
    "            GeoDataFrame: GeoDataFrame ready for saving\n",
    "    \"\"\"\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(result_df)\n",
    "    gdf.crs = current_crs\n",
    "\n",
    "    gdf.to_crs(dst_crs, inplace=True)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def save_polygons(gdf, save_path):\n",
    "    if len(gdf) == 0:\n",
    "        return\n",
    "\n",
    "    directory = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    gdf.to_file(save_path, driver='GeoJSON')\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find tile indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USER = os.getenv('NB_USER')\n",
    "BASE = f\"/home/{NB_USER}/work\"\n",
    "\n",
    "API_KEY = os.path.join(BASE, \".secret/sentinel2_google_api_key.json\")\n",
    "LOAD_DIR = os.path.join(BASE, \"satellite_imagery\")\n",
    "RESULTS_DIR = os.path.join(BASE, \"results/pbdnn\")\n",
    "PBD_DIR = os.path.join(BASE, \"notebooks/pbdnn\")\n",
    "\n",
    "BANDS = {'TCI', 'B08'}\n",
    "CONSTRAINTS = {'NODATA_PIXEL_PERCENTAGE': 15.0, 'CLOUDY_PIXEL_PERCENTAGE': 15.0, }\n",
    "PRODUCT_TYPE = 'L2A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "ukr_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/custom.geo.json\")\n",
    "if os.path.exists(ukr_shapefile):\n",
    "    oh_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/ohio.geojson\")\n",
    "    in_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/indiana.geojson\")\n",
    "    il_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/illinois.geojson\")\n",
    "    config_file = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/code/config.yaml\")\n",
    "    API_KEY = os.path.join(BASE, \"data/notebooks/pbdnn/sentinel2_google_api_key.json\")\n",
    "    PBD_DIR = os.path.join(BASE, \"data/notebooks/pbdnn\")\n",
    "    sentinel_tiles_path = os.path.join(BASE, \"data/notebooks/pbdnn/sentinel2grid.geojson\")\n",
    "    local = True\n",
    "else:\n",
    "    ukr_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/custom.geo.json\")\n",
    "    oh_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/ohio.geojson\")\n",
    "    in_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/indiana.geojson\")\n",
    "    il_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/states_shapes/illinois.geojson\")\n",
    "    config_file = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/code/config.yaml\")\n",
    "    sentinel_tiles_path = os.path.join(BASE, \"notebooks/pbdnn/sentinel2grid.geojson\")\n",
    "    local = False\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check location before filtering non-agricultural lands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {'notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/ukr_non_agriculture.geojson': ukr_shapefile,\n",
    "             'notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/ohio_shape.geojson': oh_shapefile,\n",
    "             'notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/indiana_shape.geojson': in_shapefile,\n",
    "             'notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/illinois_shape.geojson': il_shapefile\n",
    "            }\n",
    "filter_path = None\n",
    "aoi = gpd.read_file(aoi_filename)\n",
    "\n",
    "for filters_, location in locations.items():\n",
    "    loc = gpd.read_file(location)\n",
    "    if aoi.intersects(loc)[0]:\n",
    "        if local:\n",
    "            filter_path = os.path.join(BASE, \"data\", filters_)\n",
    "        else:\n",
    "            filter_path = os.path.join(BASE, filters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_folder(tile_folder, file, limit, nodata):\n",
    "    with rasterio.open(os.path.join(tile_folder, file)) as src:              \n",
    "        # Read in image as a numpy array\n",
    "        array = src.read(1)\n",
    "        # Count the occurance of NoData values in np array\n",
    "        nodata_count = np.count_nonzero(array == nodata)\n",
    "        # Get a % of NoData pixels\n",
    "        nodata_percentage = round(nodata_count / array.size * 100, 2)\n",
    "        if nodata_percentage <= limit:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata(loadings, product_type, limit=15.0, nodata=0):\n",
    "    filtered = dict()          \n",
    "    \n",
    "    for tile, folders in loadings.items():\n",
    "        filtered_folders = set()\n",
    "        for folder in folders:\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(\".jp2\") and \"OPER\" not in file:\n",
    "                    if product_type == 'L1C' and limit:\n",
    "                         if _check_folder(folder, file, limit, nodata):\n",
    "                            filtered_folders.add(folder)\n",
    "                            break\n",
    "                    else:\n",
    "                        filtered_folders.add(folder)\n",
    "        filtered[tile] = filtered_folders\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(api_key, tiles, start_date, end_date, output_dir, product_type=\"L2A\"):\n",
    "    loader = Sentinel2Downloader(api_key)\n",
    "    loadings = dict()\n",
    "    for tile in tiles:\n",
    "\n",
    "        loaded = loader.download(product_type,\n",
    "                                 [tile],\n",
    "                                 start_date=start_date,\n",
    "                                 end_date=end_date,\n",
    "                                 output_dir=output_dir,                       \n",
    "                                 bands=BANDS,\n",
    "                                constraints=CONSTRAINTS)\n",
    "        print(f'{tile} loaded')\n",
    "        \n",
    "        loadings[tile] = loaded\n",
    "    \n",
    "    tile_folders = dict()\n",
    "    for tile, tile_paths in loadings.items():\n",
    "        tile_folders[tile] = {str(Path(tile_path[0]).parent) for tile_path in tile_paths}\n",
    "    return tile_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_regex = r'\\<CLOUDY_PIXEL_PERCENTAGE\\>[0-9]*\\.?[0-9]*</CLOUDY_PIXEL_PERCENTAGE>'\n",
    "\n",
    "def get_min_clouds(loadings, max_ptc=5):\n",
    "    filtered = dict()\n",
    "    min_ptc = max_ptc\n",
    "    \n",
    "    for tile, folders in loadings.items():\n",
    "        filtered_folders = set()\n",
    "        for folder in folders:\n",
    "            for file in os.listdir(folder):\n",
    "                \n",
    "                if \"MTD_TL.xml\" in file: # MTD_TL.xml\n",
    "                    \n",
    "                    with open(os.path.join(folder, file)) as f:\n",
    "                        ptc = f.read()\n",
    "                        ptc = re.search(cloud_regex, ptc)\n",
    "                        \n",
    "                        if ptc is not None:\n",
    "                            ptc = ''.join([x for x in ptc.group(0) if x.isdigit() or x=='.'])\n",
    "                            filtered_folders.add((ptc, folder))\n",
    "                            \n",
    "                else:\n",
    "                    filtered_folders.add(('50', folder))\n",
    "\n",
    "        filtered[tile] = sorted(filtered_folders)[0][1]\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit for baseline: work/notebooks/pw/raster_predict.ipynb\n",
    "def create_style():\n",
    "    style = {'color': '#C0C0C0', 'stroke': 'e80e27', 'stroke-width': 2}\n",
    "\n",
    "    return str(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config['device'] = 'cpu'\n",
    "\n",
    "if local:\n",
    "    model_path = os.path.join(\n",
    "        BASE, 'data/notebooks/pbdnn/sip_plot_boundary_detection_nn/models/HighResolutionNet32_none_15_nir_tci_multitemporal.pth')\n",
    "else:\n",
    "    model_path = os.path.join(\n",
    "        BASE, 'notebooks/pbdnn/sip_plot_boundary_detection_nn/models/HighResolutionNet32_none_15_nir_tci_multitemporal.pth')\n",
    "\n",
    "model = load_model(config['model'].lower(), model_path, config['device'], channels=4)\n",
    "    \n",
    "print('Model loaded successfully')\n",
    "    \n",
    "origin_name = os.path.basename(aoi_filename).replace(\".geojson\", \"\")\n",
    "df = pd.DataFrame({\n",
    "    'aoi_fp': [''],\n",
    "    'b08_tile_path': [''],\n",
    "    'filters_fp': [''],\n",
    "    'geom_fp': [''],\n",
    "    'tci_tile_path': ['']\n",
    "})\n",
    "detector = BoundaryDetector(model, df, tiles_dir=f'/home/{NB_USER}/work/satellite_imagery')\n",
    "print('Detector is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([])\n",
    "\n",
    "if filter_path is not None:\n",
    "    filters = gpd.read_file(filter_path)\n",
    "    aoi = gpd.read_file(aoi_filename).to_crs(filters.crs)\n",
    "            \n",
    "    if 'ukr_non_agriculture' in filter_path:\n",
    "        filters['geometry'] = filters.buffer(0)\n",
    "\n",
    "    filters = filters[filters.intersects(aoi.geometry.values[0])]\n",
    "    if len(filters) > 1:\n",
    "        df_crs = filters.crs\n",
    "        filters = gpd.GeoDataFrame(\n",
    "            {'geometry': [filters[filters.geometry.is_valid].unary_union]})\n",
    "        filters.crs = df_crs\n",
    "        \n",
    "    print('filters are loaded')\n",
    "else:\n",
    "    filters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = START_DATE.split('-')[0]\n",
    "\n",
    "starts = [f'{year}-06-01', f'{year}-06-08', f'{year}-06-15', f'{year}-06-22', \n",
    "          f'{year}-07-01', f'{year}-07-08', f'{year}-07-15', f'{year}-07-22']\n",
    "\n",
    "ends = [f'{year}-06-07', f'{year}-06-14', f'{year}-06-21', f'{year}-06-30', \n",
    "        f'{year}-07-07', f'{year}-07-14', f'{year}-07-21', f'{year}-07-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_tiles(paths, out_raster_path='test.tif'):\n",
    "    tiles = []\n",
    "    tmp_files = []\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        if i == 0:\n",
    "            file = rasterio.open(path)\n",
    "            meta, crs = file.meta, file.crs\n",
    "        else:\n",
    "            tmp_path = path.replace(\n",
    "                '.jp2', '_tmp.jp2').replace('.tif', '_tmp.tif')\n",
    "            crs_transformed = transform_crs(path, tmp_path, \n",
    "                                            dst_crs=crs, \n",
    "                                            resolution=None)\n",
    "            tmp_files.append(crs_transformed)\n",
    "            file = rasterio.open(crs_transformed)\n",
    "        tiles.append(file)\n",
    "            \n",
    "    tile_arr, transform = merge(tiles, method='last')\n",
    "    \n",
    "    \n",
    "    meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": tile_arr.shape[1],\n",
    "                 \"width\": tile_arr.shape[2],\n",
    "                 \"transform\": transform,\n",
    "                 \"crs\": crs})\n",
    "    \n",
    "    if '.jp2' in out_raster_path:\n",
    "        out_raster_path = out_raster_path.replace('.jp2', '_merged.tif')\n",
    "    else:\n",
    "        out_raster_path = out_raster_path.replace('.tif', '_merged.tif')\n",
    "    print(f'saved raster {out_raster_path}')\n",
    "\n",
    "    for tile in tiles:\n",
    "        tile.close()\n",
    "        \n",
    "    for tmp_file in tmp_files:\n",
    "        try:\n",
    "            os.remove(tmp_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Tile {tmp_file} was removed or renamed, skipping')\n",
    "        \n",
    "    with rasterio.open(out_raster_path, \"w\", **meta) as dst:\n",
    "        dst.write(tile_arr)\n",
    "    \n",
    "    return out_raster_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_img_pred(images):\n",
    "    out_path = images[0].replace('.tif', '_mean.tif')\n",
    "    to_stack = []\n",
    "    for img in images:\n",
    "        with rasterio.open(img) as src:\n",
    "            img_arr = src.read(1)\n",
    "            to_stack.append(img_arr)\n",
    "            meta=src.meta\n",
    "            \n",
    "    mean_img = np.stack(to_stack, axis=-1)\n",
    "    mean_img = np.mean(mean_img, axis=-1)\n",
    "    \n",
    "    with rasterio.open(out_path, 'w', **meta) as dst:\n",
    "        dst.write(mean_img.astype(np.uint8), 1)\n",
    "        \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running boundary detection and filtration of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_rasters = []\n",
    "all_dirs = []\n",
    "\n",
    "for i in range(len(starts)):\n",
    "    tci_rasters, b08_rasters = [], []\n",
    "    start_date, end_date = starts[i], ends[i]\n",
    "    \n",
    "    date_tile_info = get_tiles(aoi_filename, sentinel_tiles_path)\n",
    "    loadings = load_images(API_KEY, date_tile_info.tileID.values, start_date, end_date, LOAD_DIR, PRODUCT_TYPE)\n",
    "    checked = check_nodata(loadings, PRODUCT_TYPE)\n",
    "    \n",
    "    try:\n",
    "        checked = get_min_clouds(checked)\n",
    "    except Exception:\n",
    "        print(f'No clean raster found for period from {start_date} to {end_date}, skipping')\n",
    "        continue\n",
    "    \n",
    "    for i, tile in date_tile_info.iterrows():\n",
    "\n",
    "        try:\n",
    "            tile_folder = Path(checked[tile.tileID])\n",
    "            print(f'filtered: {tile_folder}')\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            continue\n",
    "\n",
    "        full_tci_tile = [os.path.join(tile_folder, filename) for filename in os.listdir(tile_folder) if 'TCI_10m.jp2' in filename]\n",
    "        full_b08_tile = [os.path.join(tile_folder, filename) for filename in os.listdir(tile_folder) if 'B08_10m.jp2' in filename]\n",
    "        tci_rasters.append(full_tci_tile[0])\n",
    "        b08_rasters.append(full_b08_tile[0])\n",
    "        \n",
    "        all_dirs.append(tile_folder)\n",
    "\n",
    "    print(f'rasters to be processed: {len(tci_rasters)}')\n",
    "    \n",
    "    if len(tci_rasters)>1:\n",
    "        raster_path_tci = stitch_tiles(tci_rasters)\n",
    "        raster_path_b08 = stitch_tiles(b08_rasters)\n",
    "    elif len(tci_rasters)==1:\n",
    "        raster_path = tci_rasters[0]\n",
    "    elif len(tci_rasters)==0:\n",
    "        print('WARNING: no rasters were found!')\n",
    "        continue\n",
    "        \n",
    "    if '.jp2' in raster_path:\n",
    "        out_raster = raster_path.replace('.jp2', '_prediction.tif')\n",
    "    else:\n",
    "        out_raster = raster_path.replace('.tif', '_prediction.tif')\n",
    "        \n",
    "    raster_dir = os.path.join(*os.path.split(raster_path)[:-1])\n",
    "\n",
    "    pred_tif_path = detector.raster_prediction(raster_dir=raster_dir,\n",
    "                                               out_raster_path=out_raster,\n",
    "                                               aoi_path=aoi_filename,\n",
    "                                               conf_thresh=config['threshold'],\n",
    "                                               bands=['NIR', 'TCI'])\n",
    "    ens_rasters.append(pred_tif_path)\n",
    "ens_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_geom = aoi_filename.replace('_aoi.', '_prediction.')\n",
    "ens_raster = mean_img_pred(ens_rasters)\n",
    "\n",
    "polygons = detector.process_raster_predictions(ens_raster,\n",
    "                                               shapes_path=out_geom,\n",
    "                                               aoi_path=aoi_filename, \n",
    "                                               conf_thresh=config['threshold'],\n",
    "                                               min_poly_area=10e3)\n",
    "polygons = gpd.GeoDataFrame(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filters is not None:\n",
    "    try:\n",
    "        polys = filter_sindex(polygons, filters)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        filters['geometry'] = filters.buffer(0)\n",
    "        polys = filter_polygons(polygons, filters)\n",
    "    target_crs = filters.crs\n",
    "else:\n",
    "    print('Non-agricultural data is not found for a given AOI, proceeding without filtering')\n",
    "    polys = polygons\n",
    "    target_crs = polys.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"geometry\": polys.geometry}).reset_index()\n",
    "df[\"id\"] = pd.Series(map(lambda x: f\"{origin_name}_{tile.tileID}_{x}\", df.index.values))\n",
    "df[\"tileID\"] = tile.tileID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, df])\n",
    "gdf = process_polygons(result_df, target_crs)\n",
    "gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving polygons into results folder and adding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_suffix = \".temp\"\n",
    "gdf['style'] = create_style()\n",
    "save_path = os.path.join(RESULTS_DIR, f\"{REQUEST_ID}_{START_DATE}_{END_DATE}.geojson{tmp_suffix}\")\n",
    "save_polygons(gdf, save_path)\n",
    "\n",
    "try:\n",
    "    with open(save_path) as file:\n",
    "        geoms = json.load(file)\n",
    "except Exception:\n",
    "    geoms = {}\n",
    "    \n",
    "geoms['end_date'] = END_DATE\n",
    "geoms['start_date'] = START_DATE\n",
    "geoms['name'] = \"Fields' boundaries\"\n",
    "geoms['request_id'] = REQUEST_ID\n",
    "\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(geoms, file)\n",
    "os.rename(save_path, save_path[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(aoi_filename)\n",
    "except FileNotFoundError:\n",
    "    print('No helping geojson files were generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(out_raster)\n",
    "except FileNotFoundError:\n",
    "    print('No helping raster files were generated')\n",
    "    \n",
    "try:\n",
    "    os.remove(ens_raster)\n",
    "except FileNotFoundError:\n",
    "    print('No helping raster files were generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ens_rasters) > 0:\n",
    "    for raster in ens_rasters:\n",
    "        try:\n",
    "            os.remove(raster)\n",
    "        except FileNotFoundError:\n",
    "            print('No helping prediction raster found, skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in all_dirs:\n",
    "    for file in os.listdir(folder):\n",
    "        try:\n",
    "            os.remove(os.path.join(folder, file))\n",
    "        except IsADirectoryError:\n",
    "            for f in os.listdir(os.path.join(folder, file)):\n",
    "                os.remove(os.path.join(folder, file, f))\n",
    "            os.rmdir(os.path.join(folder, file))\n",
    "        \n",
    "    try:\n",
    "        os.rmdir(folder)\n",
    "    except FileNotFoundError:\n",
    "        print('No directories with rasters found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
