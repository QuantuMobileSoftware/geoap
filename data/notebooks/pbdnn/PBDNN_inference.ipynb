{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOI = 'POLYGON ((29.93584944158209 48.49763260054361, 29.92222752138016 48.19522597206074, 29.59257705249343 48.20884789226267, 29.60347458865497 48.51670328882631, 29.93584944158209 48.49763260054361))'\n",
    "START_DATE = \"2020-05-01\"\n",
    "END_DATE = \"2020-06-30\"\n",
    "REQUEST_ID = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting boundaries for given AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "import tempfile\n",
    "import shapely\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import join, basename, split\n",
    "from skimage import measure\n",
    "from scipy.ndimage import rotate\n",
    "from rasterio.features import rasterize, shapes\n",
    "from shapely.geometry import Polygon, shape, LinearRing\n",
    "import shapely.wkt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "from sentinel2download.downloader import Sentinel2Downloader\n",
    "from sip_plot_boundary_detection_nn.code.preprocessing import (\n",
    "    preprocess_sentinel_raw_data, read_raster, extract_tci)\n",
    "from sip_plot_boundary_detection_nn.code.engine import *\n",
    "from sip_plot_boundary_detection_nn.code.dataset import BoundaryDetector\n",
    "from sip_plot_boundary_detection_nn.code.filter_polygons import filter_polygons\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda as cuda\n",
    "cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_crs = 'EPSG:4326'\n",
    "\n",
    "polygon = shapely.wkt.loads(AOI)\n",
    "aoi_filename = f\"{time.time()}_aoi.geojson\"\n",
    "gpd.GeoDataFrame(gpd.GeoSeries([polygon]), columns=[\"geometry\"]).to_file(aoi_filename, driver=\"GeoJSON\")\n",
    "start_date = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(END_DATE, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(aoi_path, sentinel_tiles_path):\n",
    "    '''\n",
    "    Returns Sentinel-2 tiles that intersects with specified AoI.\n",
    "\n",
    "        Parameters:\n",
    "            aoi_path (str): Path to geojson/shp file with AoI to process.\n",
    "            sentinel_tiles_path (str): Path to geojson/shp file with all Sentinel-2 tiles.\n",
    "\n",
    "        Returns:\n",
    "            date_tile_info (GeoDataFrame): Filtered tiles (tileID, geometry, date).\n",
    "    '''\n",
    "    aoi_file = gpd.read_file(aoi_path)\n",
    "    sentinel_tiles = gpd.read_file(sentinel_tiles_path)\n",
    "    sentinel_tiles.set_index(\"Name\", drop=False, inplace=True)\n",
    "\n",
    "    best_interseciton = {\"tileID\": [], \"geometry\": []}\n",
    "    rest_aoi = aoi_file.copy()\n",
    "\n",
    "    while rest_aoi.area.sum() > 0:\n",
    "        res_intersection = gpd.overlay(rest_aoi, sentinel_tiles, how=\"intersection\")\n",
    "        biggest_area_idx = res_intersection.area.argmax()\n",
    "\n",
    "        tileID = res_intersection.loc[biggest_area_idx, \"Name\"]\n",
    "        this_aoi = res_intersection.loc[biggest_area_idx, \"geometry\"]\n",
    "\n",
    "        best_interseciton[\"tileID\"].append(tileID)\n",
    "        best_interseciton[\"geometry\"].append(this_aoi)\n",
    "\n",
    "        biggest_intersection = sentinel_tiles.loc[[tileID]]\n",
    "        rest_aoi = gpd.overlay(rest_aoi, biggest_intersection, how=\"difference\")\n",
    "        sentinel_tiles = sentinel_tiles.loc[res_intersection[\"Name\"]]\n",
    "\n",
    "    date_tile_info = gpd.GeoDataFrame(best_interseciton)\n",
    "    date_tile_info.crs = aoi_file.crs\n",
    "    \n",
    "    return date_tile_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_polygons(result_df, current_crs, limit=500, dst_crs=\"EPSG:4326\"):\n",
    "    \"\"\"\n",
    "    Prepare result Dataframe with polygons\n",
    "\n",
    "        Parameters:\n",
    "            result_df (pd.DataFrame): Result DataFrame\n",
    "            limit (int): min area for polygon in m2\n",
    "        Returns:\n",
    "            GeoDataFrame: GeoDataFrame ready for saving\n",
    "    \"\"\"\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(result_df)\n",
    "    gdf.crs = current_crs\n",
    "\n",
    "    gdf.to_crs(dst_crs, inplace=True)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def save_polygons(gdf, save_path):\n",
    "    if len(gdf) == 0:\n",
    "        return\n",
    "\n",
    "    directory = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    save_path = save_path + \".temp\" \n",
    "    gdf.to_file(save_path, driver='GeoJSON')\n",
    "    os.rename(save_path, save_path[:-5])\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find tile indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NB_USER = os.getenv('NB_USER')\n",
    "BASE = f\"/home/{NB_USER}/work\"\n",
    "\n",
    "API_KEY = os.path.join(BASE, \".secret/sentinel2_google_api_key.json\")\n",
    "LOAD_DIR = os.path.join(BASE, \"satellite_imagery\")\n",
    "RESULTS_DIR = os.path.join(BASE, \"results/pbdnn\")\n",
    "PBD_DIR = os.path.join(BASE, \"notebooks/pbdnn\")\n",
    "\n",
    "BANDS = {'TCI'}\n",
    "CONSTRAINTS = {'NODATA_PIXEL_PERCENTAGE': 15.0, 'CLOUDY_PIXEL_PERCENTAGE': 40.0, }\n",
    "PRODUCT_TYPE = 'L2A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "if local:\n",
    "    ukr_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/custom.geo.json\")\n",
    "    usa_shapefile = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/custom.geo.json\")\n",
    "    config_file = os.path.join(BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/code/config.yaml\")\n",
    "    API_KEY = os.path.join(BASE, \"data/notebooks/pbdnn/sentinel2_google_api_key.json\")\n",
    "    PBD_DIR = os.path.join(BASE, \"data/notebooks/pbdnn\")\n",
    "else:\n",
    "    ukr_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/custom.geo.json\")\n",
    "    usa_shapefile = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/custom.geo.json\")\n",
    "    config_file = os.path.join(BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/code/config.yaml\")\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check location before filtering non-agricultural lands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine = gpd.read_file(ukr_shapefile)\n",
    "usa = gpd.read_file(usa_shapefile)\n",
    "aoi = gpd.read_file(aoi_filename)\n",
    "\n",
    "if local:\n",
    "    if aoi.intersects(ukraine)[0]:\n",
    "        filter_path = os.path.join(\n",
    "            BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/ukr_non_agriculture.geojson\")\n",
    "    elif aoi.intersects(usa)[0]:\n",
    "        filter_path = os.path.join(\n",
    "            BASE, \"data/notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/us_shape.geojson\")\n",
    "    else:\n",
    "        filter_path = None\n",
    "else:\n",
    "    if aoi.intersects(ukraine)[0]:\n",
    "        filter_path = os.path.join(\n",
    "            BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/ukr_shapes/ukr_non_agriculture.geojson\")\n",
    "    elif aoi.intersects(usa)[0]:\n",
    "        filter_path = os.path.join(\n",
    "            BASE, \"notebooks/pbdnn/sip_plot_boundary_detection_nn/usa_shapes/us_shape.geojson\")\n",
    "    else:\n",
    "        filter_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_folder(tile_folder, file, limit, nodata):\n",
    "    with rasterio.open(os.path.join(tile_folder, file)) as src:              \n",
    "        # Read in image as a numpy array\n",
    "        array = src.read(1)\n",
    "        # Count the occurance of NoData values in np array\n",
    "        nodata_count = np.count_nonzero(array == nodata)\n",
    "        # Get a % of NoData pixels\n",
    "        nodata_percentage = round(nodata_count / array.size * 100, 2)\n",
    "        if nodata_percentage <= limit:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodata(loadings, product_type, limit=15.0, nodata=0):\n",
    "    filtered = dict()          \n",
    "    \n",
    "    for tile, folders in loadings.items():\n",
    "        filtered_folders = set()\n",
    "        for folder in folders:\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(\".jp2\") and \"OPER\" not in file:\n",
    "                    if product_type == 'L1C' and limit:\n",
    "                         if _check_folder(folder, file, limit, nodata):\n",
    "                            filtered_folders.add(folder)\n",
    "                            break\n",
    "                    else:\n",
    "                        filtered_folders.add(folder)\n",
    "        filtered[tile] = filtered_folders\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_date(loadings):\n",
    "    def _find_last_date(folders):        \n",
    "        dates = list()\n",
    "        for folder in folders:\n",
    "            search = re.search(r\"_(\\d+)T\\d+_\", str(folder))\n",
    "            date = search.group(1)\n",
    "            date = datetime.strptime(date, '%Y%m%d')\n",
    "            dates.append(date)    \n",
    "        last_date = max(dates)\n",
    "        last_date = datetime.strftime(last_date, '%Y%m%d')\n",
    "        return last_date\n",
    "    \n",
    "    filtered = dict()\n",
    "    for tile, folders in loadings.items():\n",
    "        \n",
    "        try:\n",
    "            last_date = _find_last_date(folders)\n",
    "            for folder in folders:\n",
    "                if last_date in folder:\n",
    "                    filtered[tile] = folder\n",
    "        except Exception as ex:\n",
    "            pass\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(api_key, tiles, start_date, end_date, output_dir, product_type=\"L2A\"):\n",
    "    loader = Sentinel2Downloader(api_key)\n",
    "    loadings = dict()\n",
    "    for tile in tiles:\n",
    "\n",
    "        loaded = loader.download(product_type,\n",
    "                                 [tile],\n",
    "                                 start_date=start_date,\n",
    "                                 end_date=end_date,\n",
    "                                 output_dir=output_dir,                       \n",
    "                                 bands=BANDS,\n",
    "                                constraints=CONSTRAINTS)\n",
    "        \n",
    "        loadings[tile] = loaded\n",
    "    \n",
    "    tile_folders = dict()\n",
    "    for tile, tile_paths in loadings.items():\n",
    "        tile_folders[tile] = {str(Path(tile_path[0]).parent) for tile_path in tile_paths}\n",
    "    return tile_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit for baseline: work/notebooks/pw/raster_predict.ipynb\n",
    "def create_style(class_):\n",
    "    colors = dict(boundary='#e80e27')\n",
    "    \n",
    "    style = dict(color=colors.get(class_.lower(), '#C0C0C0'),\n",
    "                stroke='#e80e27')\n",
    "    style['stroke-width'] = 2\n",
    "    return str(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_tiles_path = \"sentinel2grid.geojson\"\n",
    "model = make_unet_plusplus()\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if local:\n",
    "    config['model_weights_path'] = os.path.join(\n",
    "        BASE, 'data/notebooks/pbdnn/sip_plot_boundary_detection_nn/models/chkpt_UnetPlusPlus_imagenet_200_new_dataset_v3.pt')\n",
    "else:\n",
    "    config['model_weights_path'] = os.path.join(\n",
    "        BASE, 'notebooks/pbdnn/sip_plot_boundary_detection_nn/models/chkpt_UnetPlusPlus_imagenet_200_new_dataset_v3.pt')\n",
    "\n",
    "if config['device'] == 'cuda':\n",
    "    model.load_state_dict(torch.load(config['model_weights_path']))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\n",
    "        config['model_weights_path'],map_location=torch.device('cpu')))\n",
    "    \n",
    "date_tile_info = get_tiles(aoi_filename, sentinel_tiles_path)\n",
    "loadings = load_images(API_KEY, date_tile_info.tileID.values, START_DATE, END_DATE, LOAD_DIR, PRODUCT_TYPE)\n",
    "checked = check_nodata(loadings, PRODUCT_TYPE)\n",
    "filtered = filter_date(checked)\n",
    "    \n",
    "origin_name = os.path.basename(aoi_filename).replace(\".geojson\", \"\")\n",
    "\n",
    "detector = BoundaryDetector(model, tiles_dir=f'/home/{NB_USER}/work/satellite_imagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([])\n",
    "\n",
    "if filter_path is not None:\n",
    "    filters = gpd.read_file(filter_path)\n",
    "    aoi = gpd.read_file(aoi_filename).to_crs(filters.crs)\n",
    "            \n",
    "    if 'ukr_non_agriculture' in filter_path:\n",
    "        filters['geometry'] = filters.buffer(0)\n",
    "            \n",
    "    filters = filters[filters.intersects(aoi.geometry.values[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.merge import merge\n",
    "\n",
    "def stitch_tiles(paths, out_raster_path='test.tif'):\n",
    "    tiles = []\n",
    "    \n",
    "    for i, p in enumerate(paths):\n",
    "        file = rasterio.open(p)\n",
    "        tiles.append(file)\n",
    "            \n",
    "        \n",
    "    tile_arr, transform = merge(tiles, method='last')\n",
    "    meta, crs = file.meta, file.crs\n",
    "    \n",
    "    meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": tile_arr.shape[1],\n",
    "                 \"width\": tile_arr.shape[2],\n",
    "                 \"transform\": transform,\n",
    "                 \"crs\": crs})\n",
    "    \n",
    "    out_raster_path = out_raster_path.replace('.jp2', '.tif')\n",
    "\n",
    "    for t in tiles:\n",
    "        t.close()\n",
    "        \n",
    "    with rasterio.open(out_raster_path, \"w\", **meta) as dst:\n",
    "        dst.write(tile_arr)\n",
    "    \n",
    "    return out_raster_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory(dir=PBD_DIR) as tmpdirname:\n",
    "    rasters = []\n",
    "    for i, tile in date_tile_info.iterrows():\n",
    "        try:\n",
    "            tile_folder = Path(filtered[tile.tileID])\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "\n",
    "        full_tile = [os.path.join(tile_folder, x) for x in os.listdir(tile_folder) if x.endswith('.jp2')]\n",
    "        rasters.append(full_tile[0])\n",
    "        \n",
    "    if len(rasters)>1:\n",
    "        raster_path = stitch_tiles(rasters)\n",
    "    elif len(rasters)==0:\n",
    "        raster_path = rasters[0]\n",
    "            \n",
    "    out_raster = raster_path.replace('.tif', '_prediction.tif')\n",
    "    out_geom = aoi_filename.replace('_aoi.', '_prediction.')\n",
    "        \n",
    "    start_raster = time.time()\n",
    "    pred_tif_path = detector.raster_prediction(in_raster_path=raster_path,\n",
    "                                                out_raster_path=out_raster,\n",
    "                                                aoi_path=aoi_filename,\n",
    "                                                conf_thresh=0.25)\n",
    "        \n",
    "    start_poly = time.time()\n",
    "    polygons = detector.process_raster_predictions(pred_tif_path,\n",
    "                                                    shapes_path=out_geom,\n",
    "                                                    aoi_path=aoi_filename, \n",
    "                                                    conf_thresh=0.25)\n",
    "    polygons = gpd.GeoDataFrame(polygons)\n",
    "\n",
    "    try:\n",
    "        polys = filter_polygons(polygons, filters)\n",
    "    except Exception:\n",
    "        filters['geometry'] = filters.buffer(0)\n",
    "        polys = filter_polygons(polygons, filters)\n",
    "            \n",
    "    df = pd.DataFrame({\"geometry\": polys.geometry}).reset_index()\n",
    "    df[\"id\"] = pd.Series(map(lambda x: f\"{origin_name}_{tile.tileID}_{x}\", df.index.values))\n",
    "    df[\"tileID\"] = tile.tileID\n",
    "    df[\"start_date\"] = START_DATE\n",
    "    df[\"end_date\"] = END_DATE\n",
    "\n",
    "    result_df = pd.concat([result_df, df])\n",
    "\n",
    "gdf = process_polygons(result_df, filters.crs)\n",
    "gdf['class_'] = 'boundary'\n",
    "gdf['style'] = gdf.class_.apply(lambda cl: create_style(cl))\n",
    "save_path = os.path.join(RESULTS_DIR, f\"{origin_name}_prediction.geojson\")\n",
    "save_polygons(gdf, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
